+++
# Experience widget.
widget = "experience"  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 40  # Order that this section will appear.

title = "Experiencia Laboral"
subtitle = ""

# Date format for experience
#   Refer to https://sourcethemes.com/academic/docs/customization/#date-format
date_format = "Jan 2006"

# Experiences.
#   Add/remove as many `[[experience]]` blocks below as you like.
#   Required fields are `title`, `company`, and `date_start`.
#   Leave `date_end` empty if it's your current employer.
#   Begin/end multi-line descriptions with 3 quotes `"""`.

[[experience]]
  title = "Senior Data Engineer"
  company = "Marley Spoon GA"
  company_url = "https://www.marleyspoon.de/"
  location = "Berlin, Alemania"
  date_start = "2020-10-01"
  date_end = ""
  description = """ 
  
  Acabo de unirme al equipo digital como Senior Data Engineer.
  
  Responsabilidades incluyen:
  
  * Desarrollo de productos - Sistema de recomendacion y predicción de ventas.
  * Desarrollo de ETL con Apache Airflow, SQL y Python.
  * Data warehousing y modelamiento de datos
  * Re-ingeniería de código legacy 
  
  """

[[experience]]
  title = "Manager de Ingeniería"
  company = "YGroup Companies"
  company_url = "https://www.ygroupcompanies.com/"
  location = "Amsterdam, Holanda"
  date_start = "2020-03-01"
  date_end = "2020-10-01"
  description = """
  
  Estuve a cargo de la gestión de un equipo de ingeniería con el que construimos una plataforma de datos para Carlsberg Asia, que sirve como punto central de analitica e inteligencia de negocios para los departamentos de supply chain, marketing, finanzas y ventas.
  
  La plataforma y su infrastructura la desarrollamos con Mircosoft Azure y Databricks.
  
  Nos basamos en una arquitectura de auto-servicio y distribuida, donde los diferentes departamentos podian seleccionar sus fuentes de datos y la forma en que querian consumirlos y en el backend, nosotros teniamos la flexibilidad de mover los datos desde su fuente --> data lake --> DWH y finalmente a una capa semántica para ser consumida en PowerBI.
  Utilizamos pipelines genericos escritos en PySpark para aquello.
  
  El equipo está compuesto por un arquitecto de datos, un ingenierio BI, Scrum Master, dos Data Engineers y yo.
  
  Responsabilidades incluyen:
  
  * Gestión de proyectos digitales
  * Gestion del roadmap y definición de metas
  * Comunicación con stakeholders
  * Dirección y liderazgo técnico
  """

[[experience]]
  title = "Senior Data Engineer"
  company = "YGroup Companies"
  company_url = "https://www.ygroupcompanies.com/"
  location = "Amsterdam, Holanda"
  date_start = "2018-07-01"
  date_end = "2020-10-01"
  description = """
  Mi rol de consultor y de ingeniero se basaba en guiar a las empresas en la implementación de plataformas de datos, desarrollando productos en base a estos y asegurandose que el desarrollo suigiera las mejores practicas de la industria.
  
  Ayudé a tres grandes clientes durante este tiempo. 
  
  **Carlsberg Breweries A/S**
  
  Responsable por la ingenieria y la implementacion de modelos de machine learning en producción para Predicciones de Demanda y Customer Churn.
  
  El desarrollo fue una combinación de Python, R y PySpark, implementado en Databricks. 
  
  Desarrollé diferentes modules de las aplicaciones incluyendo el feature engineering, data loader y funciones de inferencia.
  
  **SHV Holdings N.V.**
  
  Responsable de construir la plataform de datos del Holding. Utilicé Microsoft Azure con un approach de data lake - DWH.  
  
  La orquestraciôn de los jobs era responsabilidad de Airflow implementado en Docker en Azure.
  
  Desarrollé 15 data pipelines que disponibilizaban los datos diaramente para equipos de Finanzas, Claims y Controllers.
  
  **Stern NL**
  
  Responsable de provisionar la infrastructura cloud para los data scientists y guiar en la implementación de analisis en Python y R.
  
  Integrar fuentes de datos a travez de APIs internas y externas mediante Apache Airflow.
  
  Optimizar los procesos de post-venta al disponibilidar la data hacia el Data Warehouse y automatizar su actualizacion diaria.
  
  Responsabilidades incluyen:

  * Diseño e implementación de plataforma de datos 
  * Flujo de Machine Learning en producción
  * Desarrollo técnico de productos de datos.
  * Integración de datos
  * Provision de Infrastructura y DevOps
  """

[[experience]]
  title = "Team Lead & Data Scientist"
  company = "Intelligens - Conversica"
  company_url = "https://www.conversica.com/"
  location = "Santiago, Chile"
  date_start = "2016-06-01"
  date_end = "2018-07-01"
  description = """
  
  I lead the team developing a natural language processing engine in the company for conversational Artificial Intelligence.
  
  Logros Técnicos:
  - Quick machine learning deployment pipeline to production for NLP and operational tasks
  - Over 95% Accuracy of models for similar tasks in production for the time period of 2 years
  - Automated more than 70% of operational/product related tasks using Machine Learning

  Responsabilidades incluyen:
  * Development a natural approach to accomplish Human-Like Conversational AI.
  * Design the feature roadmap for NLP and Natural Language Understanding into the system.
  * Build Artificial Intelligence customizable process based on user's interaction.
  * Designed, built and managed automated self-learning pipeline.
  * Train and Deploy Machine Learning and Deep Learning Models in production.
  """

[[experience]]
  title = "Data Engineer"
  company = "Equifax"
  company_url = "https://www.equifax.com/"
  location = "Santiago, Chile"
  date_start = "2015-03-01"
  date_end = "2016-07-01"
  description = """
  
  Responsabilidades incluyen:
  * Desarrollo de ETL para el análisis de metricas a nivel operacional 
  * Diseño del data warehouse en base a Fact y Dimensions
  * Traducción de requerimientos del negocio en tablas en BBDD SQL.
  * Realicé el POC de una nueva herramiento de integración de datos llamada Pentaho.
  """

+++
